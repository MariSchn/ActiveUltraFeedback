inputs_path: datasets/combined_annotations_llama_features
oracle_name: ultrafeedback
acquisition_function: random
reward_model: enn

previous_checkpoint_path: null
previous_output_path: null
report_to: wandb
debug: false

seed: &a 42
max_length: &b 4096
outer_loop_batch_size: 32

acquisition_function_config:
  random:
    seed: *a
  ultrafeedback:
    seed: *a
  dts:
    beta: 1
    max_iterations: 30
  ids:
    argmax_tol: 1e-4
    decision_buffer: 0.0
    use_candidate_set: false
  rucb:
    beta: 1
    argmax_tol: 1e-4
    decision_buffer: 0.0
    use_candidate_set: false # TODO: ask martin why rucb takes max_iterations as input

reward_model_config:
  enn:
    base_model_name_or_path: "unsloth/Qwen2.5-1.5B-Instruct"
    freeze_base_model: true
    feature_extraction_layer: "last_hidden_state"
    initialization_xavier_gain: 2.0

reward_trainer_config:
  enn:
    num_train_epochs: 1
    save_strategy: "no"
    report_to: "none"
    disable_tqdm: true
    logging_strategy: "no" # TODO: or steps? that's the default value
    logging_steps: 1
    lr_scheduler_type: "constant"
    learning_rate: 5e-6
    regularization_towards_initial_weights: 10
    warmup_steps: 0
    bf16: true
    max_length: *b

    replay_buffer_size: 3200 # factor, it's 100 times better. 1000 times  (100 * outerloopbatchsize) 10, 100, 1000
    effective_batch_size: 32
    max_training_steps: 10
    exponential_decay_base: 0.95
    regularization_weight_decay_type: linear # or constant? that's the default value

base_output_dir: datasets
base_trainer_dir: trainer_output
base_logs_dir: logs
base_wandb_dir: wandb
base_wandb_project: loop